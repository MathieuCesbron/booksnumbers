{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "from number_parser import parse_ordinal\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract numbers from a text\n",
    "def extract_numbers(text):\n",
    "    dict = {\n",
    "        \"twofold\": 2,\n",
    "        \"threefold\": 3,\n",
    "        \"fourfold\": 4,\n",
    "        \"fivefold\": 5,\n",
    "        \"sixfold\": 6,\n",
    "        \"sevenfold\": 7,\n",
    "        \"heightfold\": 8,\n",
    "        \"ninefold\": 9,\n",
    "        \"hundredfold\": 100,\n",
    "        \"thousandfold\": 1000,\n",
    "        \"tens\": 10,\n",
    "        \"hundreds\": 100,\n",
    "        \"thousands\": 1000,\n",
    "        \"millions\": 1000000,\n",
    "        \"billions\": 1000000000,\n",
    "        \"duality\": 2,\n",
    "        \"trinity\": 3,\n",
    "        \"duo\": 2,\n",
    "        \"trio\": 3,\n",
    "    }\n",
    "\n",
    "    words = text.split()\n",
    "    numbers = []\n",
    "    for word in words:\n",
    "        # Strip leading and trailing punctuation\n",
    "        word = word.strip(string.punctuation)\n",
    "        \n",
    "        try:\n",
    "            num = parse_ordinal(word, language=\"en\")\n",
    "            if word in dict:\n",
    "                num = dict[word]\n",
    "            numbers.append(num) if num else None\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 / 17868 analyzed\n",
      "30 / 17868 analyzed\n",
      "40 / 17868 analyzed\n",
      "50 / 17868 analyzed\n",
      "60 / 17868 analyzed\n",
      "70 / 17868 analyzed\n",
      "80 / 17868 analyzed\n",
      "90 / 17868 analyzed\n",
      "100 / 17868 analyzed\n",
      "110 / 17868 analyzed\n",
      "120 / 17868 analyzed\n",
      "130 / 17868 analyzed\n",
      "140 / 17868 analyzed\n",
      "150 / 17868 analyzed\n",
      "160 / 17868 analyzed\n",
      "170 / 17868 analyzed\n",
      "180 / 17868 analyzed\n",
      "190 / 17868 analyzed\n",
      "200 / 17868 analyzed\n",
      "210 / 17868 analyzed\n",
      "220 / 17868 analyzed\n",
      "230 / 17868 analyzed\n",
      "240 / 17868 analyzed\n",
      "250 / 17868 analyzed\n",
      "260 / 17868 analyzed\n",
      "270 / 17868 analyzed\n",
      "280 / 17868 analyzed\n",
      "290 / 17868 analyzed\n",
      "300 / 17868 analyzed\n",
      "310 / 17868 analyzed\n",
      "320 / 17868 analyzed\n",
      "330 / 17868 analyzed\n",
      "340 / 17868 analyzed\n",
      "350 / 17868 analyzed\n",
      "360 / 17868 analyzed\n",
      "370 / 17868 analyzed\n",
      "380 / 17868 analyzed\n",
      "390 / 17868 analyzed\n",
      "400 / 17868 analyzed\n",
      "410 / 17868 analyzed\n",
      "420 / 17868 analyzed\n",
      "430 / 17868 analyzed\n",
      "440 / 17868 analyzed\n",
      "450 / 17868 analyzed\n",
      "460 / 17868 analyzed\n",
      "470 / 17868 analyzed\n",
      "480 / 17868 analyzed\n",
      "490 / 17868 analyzed\n",
      "500 / 17868 analyzed\n",
      "510 / 17868 analyzed\n",
      "520 / 17868 analyzed\n",
      "530 / 17868 analyzed\n",
      "540 / 17868 analyzed\n",
      "550 / 17868 analyzed\n",
      "560 / 17868 analyzed\n",
      "570 / 17868 analyzed\n",
      "580 / 17868 analyzed\n",
      "590 / 17868 analyzed\n",
      "600 / 17868 analyzed\n",
      "610 / 17868 analyzed\n",
      "620 / 17868 analyzed\n",
      "630 / 17868 analyzed\n",
      "640 / 17868 analyzed\n",
      "650 / 17868 analyzed\n",
      "660 / 17868 analyzed\n",
      "670 / 17868 analyzed\n",
      "680 / 17868 analyzed\n",
      "690 / 17868 analyzed\n",
      "700 / 17868 analyzed\n",
      "710 / 17868 analyzed\n",
      "720 / 17868 analyzed\n",
      "730 / 17868 analyzed\n",
      "740 / 17868 analyzed\n",
      "750 / 17868 analyzed\n",
      "760 / 17868 analyzed\n",
      "770 / 17868 analyzed\n",
      "780 / 17868 analyzed\n",
      "790 / 17868 analyzed\n",
      "800 / 17868 analyzed\n",
      "810 / 17868 analyzed\n",
      "820 / 17868 analyzed\n",
      "830 / 17868 analyzed\n",
      "840 / 17868 analyzed\n",
      "850 / 17868 analyzed\n",
      "860 / 17868 analyzed\n",
      "870 / 17868 analyzed\n",
      "880 / 17868 analyzed\n",
      "890 / 17868 analyzed\n",
      "900 / 17868 analyzed\n",
      "910 / 17868 analyzed\n",
      "920 / 17868 analyzed\n",
      "930 / 17868 analyzed\n",
      "940 / 17868 analyzed\n",
      "950 / 17868 analyzed\n",
      "960 / 17868 analyzed\n",
      "970 / 17868 analyzed\n",
      "980 / 17868 analyzed\n",
      "990 / 17868 analyzed\n",
      "1000 / 17868 analyzed\n",
      "1010 / 17868 analyzed\n",
      "1020 / 17868 analyzed\n",
      "1030 / 17868 analyzed\n",
      "1040 / 17868 analyzed\n",
      "1050 / 17868 analyzed\n",
      "1060 / 17868 analyzed\n",
      "1070 / 17868 analyzed\n",
      "1080 / 17868 analyzed\n",
      "1090 / 17868 analyzed\n",
      "1100 / 17868 analyzed\n",
      "1110 / 17868 analyzed\n",
      "1120 / 17868 analyzed\n",
      "1130 / 17868 analyzed\n",
      "1140 / 17868 analyzed\n",
      "1150 / 17868 analyzed\n",
      "1160 / 17868 analyzed\n",
      "1170 / 17868 analyzed\n",
      "1180 / 17868 analyzed\n",
      "1190 / 17868 analyzed\n",
      "1200 / 17868 analyzed\n",
      "1210 / 17868 analyzed\n",
      "1220 / 17868 analyzed\n",
      "1230 / 17868 analyzed\n",
      "1240 / 17868 analyzed\n",
      "1250 / 17868 analyzed\n",
      "1260 / 17868 analyzed\n",
      "1270 / 17868 analyzed\n",
      "1280 / 17868 analyzed\n",
      "1290 / 17868 analyzed\n",
      "1300 / 17868 analyzed\n",
      "1310 / 17868 analyzed\n",
      "1320 / 17868 analyzed\n",
      "1330 / 17868 analyzed\n",
      "1340 / 17868 analyzed\n",
      "1350 / 17868 analyzed\n",
      "1360 / 17868 analyzed\n",
      "1370 / 17868 analyzed\n",
      "1380 / 17868 analyzed\n",
      "1390 / 17868 analyzed\n",
      "1400 / 17868 analyzed\n",
      "1410 / 17868 analyzed\n",
      "1420 / 17868 analyzed\n",
      "1430 / 17868 analyzed\n",
      "1440 / 17868 analyzed\n",
      "1450 / 17868 analyzed\n",
      "1460 / 17868 analyzed\n",
      "1470 / 17868 analyzed\n",
      "1480 / 17868 analyzed\n",
      "1490 / 17868 analyzed\n",
      "1500 / 17868 analyzed\n",
      "1510 / 17868 analyzed\n",
      "1520 / 17868 analyzed\n",
      "1530 / 17868 analyzed\n",
      "1540 / 17868 analyzed\n",
      "1550 / 17868 analyzed\n",
      "1560 / 17868 analyzed\n",
      "1570 / 17868 analyzed\n",
      "1580 / 17868 analyzed\n",
      "1590 / 17868 analyzed\n",
      "1600 / 17868 analyzed\n",
      "1610 / 17868 analyzed\n",
      "1620 / 17868 analyzed\n",
      "1630 / 17868 analyzed\n",
      "1640 / 17868 analyzed\n",
      "1650 / 17868 analyzed\n",
      "1660 / 17868 analyzed\n",
      "1670 / 17868 analyzed\n",
      "1680 / 17868 analyzed\n",
      "1690 / 17868 analyzed\n",
      "1700 / 17868 analyzed\n",
      "1710 / 17868 analyzed\n",
      "1720 / 17868 analyzed\n",
      "1730 / 17868 analyzed\n",
      "1740 / 17868 analyzed\n",
      "1750 / 17868 analyzed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "books_folder_path = \"./books1/epubtxt\"\n",
    "books_name = [f for f in os.listdir(books_folder_path)]\n",
    "\n",
    "json_file_path = \"./data.json\"\n",
    "data_json = open(json_file_path, 'r+')\n",
    "data = json.load(data_json)\n",
    "\n",
    "stats = OrderedDict()\n",
    "for key, value in data[\"stats\"].items():\n",
    "    stats[int(key)] = value\n",
    "\n",
    "\n",
    "for i in range(data[\"books_num\"], len(books_name)):\n",
    "    book_name = books_name[i]\n",
    "    book_path = os.path.join(books_folder_path, book_name)\n",
    "    \n",
    "    with open(book_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "        numbers = extract_numbers(content)\n",
    "        numbers_sorted = OrderedDict(sorted(Counter(numbers).items()))\n",
    "        for number in numbers_sorted:\n",
    "            stats[int(number)] = stats.get(int(number), 0) + numbers_sorted.get(int(number), 0)\n",
    "\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            print(f\"{i} / {len(books_name)} analyzed\")\n",
    "            data[\"books_num\"] = i\n",
    "            data[\"stats\"] = OrderedDict(sorted(Counter(stats).items()))\n",
    "            \n",
    "            data_json.seek(0)\n",
    "            json.dump(data, data_json, indent=2)\n",
    "            data_json.truncate()\n",
    "\n",
    "print(f\"{i} / {len(books_name)} analyzed\")\n",
    "data[\"books_num\"] = i\n",
    "data[\"stats\"] = OrderedDict(sorted(Counter(stats).items()))\n",
    "\n",
    "data_json.seek(0)\n",
    "json.dump(data, data_json, indent=2)\n",
    "data_json.truncate()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "booknumbers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
